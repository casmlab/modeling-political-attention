{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import csv\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../../word2vec-twitter/')\n",
    "\n",
    "# from word2vecReader import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset created by 1.0-ams-create-best-classifiers\n",
    "data_file = '../../../../data/russell/russell_processed_0.9.csv.gz'\n",
    "tweets = pd.read_csv(data_file, dtype=str, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweets):\n",
    "    \n",
    "    ## reformat data to be readable by model \n",
    "    coln = []\n",
    "    for n in tweets.text_tokenized:\n",
    "        newl = []\n",
    "        for m in n[1:-1].split():\n",
    "            newl.append(m[1:-2])\n",
    "        coln.append(newl)\n",
    "\n",
    "    tweets.text_tokenized = coln\n",
    "    documents = list(tweets.text_tokenized)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = process_data(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build vocabulary and train model, passing in a list of lists where each list within the \n",
    "## main list contains a set of tokens from a tweet. \n",
    "w2v = gensim.models.Word2Vec(\n",
    "    documents,\n",
    "    min_count=2,\n",
    "    workers=8)\n",
    "\n",
    "w2v.train(documents, total_examples=len(documents), epochs=10)\n",
    "# w2v.wv.save_word2vec_format('../../../models/w2v_format.bin')\n",
    "w2v.save('../../../../models/feature_extraction/w2v.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
