{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import matutils, corpora, models\n",
    "import gensim\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import string\n",
    "import numpy as np\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Tokenizers\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Notebook Parameters\n",
    "Here we set variables necessary for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True) # which tokenizer to use\n",
    "\n",
    "stop_words_en = get_stop_words('en') # English stoplist\n",
    "stop_words_sp = get_stop_words('spanish') # Spanish stoplist\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "for word in stop_words_en:\n",
    "    stop_words.append(word)\n",
    "for word in stop_words_sp:\n",
    "    stop_words.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "Twitter prohibits us from sharing the raw tweets, and Russell's raw data is not publicly available at this time (we received Russell's raw data via private email). As such, though we include all code we used to prepare the data to build our lda model and apply our model to for reference, we have commented out those lines which require raw datasets. \n",
    "\n",
    "The processed data that we used to build the model can be found at:\n",
    "\n",
    "    '../../../../data/unsupervised/model_source/congress115_preprocessed.csv.gz'\n",
    "    \n",
    "The processed data that we used to test the model can be found at:\n",
    "\n",
    "    '../../../../data/unsupervised/model_source/russell_preprocessed.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data we should use to build the model\n",
    "\n",
    "\n",
    "# data_file = 'RAWDATA_congress' \n",
    "# tweets = pd.read_csv(data_file, compression='gzip', dtype=str)\n",
    "\n",
    "##  data we should use apply the model to (complete set of Russell's labeled tweets)\n",
    "# data_file_r = 'RAWDATA_russell'\n",
    "# russell_tweets = pd.read_csv(data_file_r, compression='gzip', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize data using one of the following tokenizers:\n",
    "* [TweetTokenizer](http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual)\n",
    "* [RegexpTokenizer](http://www.nltk.org/api/nltk.tokenize.html?highlight=regexp#module-nltk.tokenize.regexp)\n",
    "\n",
    "With some help from [Libelli](# with help from https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    ")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_data(tweets):\n",
    "    \n",
    "    ## to train lda model\n",
    "    tweets['original'] = np.where(tweets.text.str.startswith(('RT', 'nan')), 0, 1)\n",
    "    original_tweets = tweets[tweets['original'] == 1]\n",
    "    \n",
    "    return original_tweets\n",
    "\n",
    "def process_inference_data(tweets):\n",
    "    \n",
    "    ## to make inference from trained lda model\n",
    "    tweets['RT'] = tweets.RT.astype(float)\n",
    "    original_tweets = tweets[tweets['RT'] == 0]\n",
    "    original_tweets.head()\n",
    "    \n",
    "    return original_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, stop_words, tokenizer, text_column, id_column, userid_column, output_csv):\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    doc_set = list(df[text_column])\n",
    "    \n",
    "    for doc in doc_set:\n",
    "        \n",
    "        # remove URLs\n",
    "        doc = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', doc, flags=re.MULTILINE)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        nopunct_tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # keep words length 3 or more\n",
    "        long_tokens = [token for token in nopunct_tokens if len(token) > 2]\n",
    "        \n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        alpha_tokens = [word for word in long_tokens if word.isalpha()]\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in alpha_tokens if not i in stop_words]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stopped_tokens)\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['id_str'] = df[id_column]\n",
    "    new_df['user_id'] = df[userid_column]\n",
    "    new_df['preprocessed_text'] = texts\n",
    "    \n",
    "    new_df.to_csv(output_csv, index=False, compression='gzip')\n",
    "        \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepping data to build the model, commented since we do not include \n",
    "## raw data in public repo so function fill not run\n",
    "# original_tweets = process_training_data(tweets)\n",
    "# texts = preprocess(original_tweets, stop_words, tokenizer,\n",
    "#                    'full_text', 'id_str', 'user.id_str', \n",
    "#                    '../../../../data/unsupervised/model_source/congress115_preprocessed.csv.gz')\n",
    "\n",
    "## prepping data to apply the model\n",
    "# original_tweets_r = process_inference_data(russell_tweets)\n",
    "# texts = preprocess(original_tweets_r, stop_words, tokenizer, 'text', 'KEYID', 'Name',\n",
    "#                   '../../../../data/unsupervised/model_source/russell_preprocessed.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the appropriate directory:\n",
    "\n",
    "    cd ~/Desktop/icwsm-2019/external_respositories/mallet-2.0.8\n",
    " \n",
    "\n",
    "Set up corpus upon which to train the model using the preprocessed tokens made in the preceding cell of this notebook:\n",
    "\n",
    "    ./bin/mallet import-file --input ../../data/unsupervised/model_source/congress115_preprocessed.csv.gz --keep-sequence --output ../../data/unsupervised/model_source/congress115_train.mallet\n",
    "\n",
    "\n",
    "Train the model:\n",
    "\n",
    "    ./bin/mallet train-topics --input ../data/congress115_train.mallet --inferencer-filename ../../data/unsupervised/model_output/lda_tw_50_unigram_congress115_inferencer_.mallet --num-topics 50 --output-topic-keys ../../data/unsupervised/model_output/congress115_topics_FULL.txt --optimize-interval 10 --output-model ../../data/unsupervised/model_output/lda_tw_50_unigram_congress115.model --output-state ../../data/unsupervised/model_output/congress115_topic_state_50.gz --output-topic-keys ../../data/unsupervised/model_output/congress115_keys_50.txt --output-doc-topics ../../data/unsupervised/model_output/congress115_compostion_50.txt \n",
    "\n",
    "\n",
    "Open the file \"congress115_keys.txt\" in excel, and create three columns: \"ams label\", \"policy codebook label\", and \"policy codebook book\". The first of these columns is my (ams) intial manual labeling attempt. The second of these columns is the corresponding policy codebook label if available. If not avaialable, I (ams) come up with a new label. The third of these columns is the corresponding policy codebook code number if available. If not avaialable, I (ams) come up with a new code number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Topic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the appropriate directory:\n",
    "\n",
    "    cd ~/Desktop/icwsm-2019/external_respositories/mallet-2.0.8\n",
    " \n",
    "\n",
    "Using corpus created in training of the model to ensure compatibility of new dataset (i.e. '--use-pipe-from' field), import documents into mallet format:\n",
    "\n",
    "    ./bin/mallet import-file --input ../../data/unsupervised/model_source/russell_preprocessed.csv.gz --keep-sequence --output ../../data/unsupervised/model_source/russell_infer.mallet --use-pipe-from ../../data/unsupervised/model_source/congress115_train.mallet\n",
    "\n",
    "Using the appropriate model inferencer (i.e. the '--inferencer' field), infer the topic distribution (i.e. the '--output-doc-topics' field) for a set of documents prepared in the preceding step (i.e. the '--input' field):\n",
    "\n",
    "    ./bin/mallet infer-topics --input ../../data/unsupervised/model_source/russell_infer.mallet --inferencer ../../models/best/lda/lda_tw_50_unigram_congress115_inferencer.mallet --output-doc-topics ../../data/unsupervised/model_output/russell-topic-composition.txt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
